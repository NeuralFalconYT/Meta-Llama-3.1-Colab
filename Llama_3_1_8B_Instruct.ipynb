{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to access ```Llama-3.1-8B-Instruct```\n",
        "## Step 1:\n",
        "visit [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) Page <br>\n",
        "## Step 2:\n",
        "Wait a few minutes, you will receive access from Meta.<br>\n",
        "## Step 3:\n",
        "[Access Hugging Face Token](https://huggingface.co/settings/tokens)<br>\n",
        "##Step 4:\n",
        "Copy paste Hugging Face Token in Google Colab\n",
        "\n",
        "![](https://raw.githubusercontent.com/NeuralFalconYT/Meta-Llama-3.1-Colab/main/colab.jpg)\n"
      ],
      "metadata": {
        "id": "_s8Kva6gbGlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install required packages and Auto - Restart Session\n",
        "!pip install accelerate\n",
        "!pip install --upgrade transformers\n",
        "!pip install gradio==4.36.1\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "import time\n",
        "time.sleep(5)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mT5oEfQOakK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import & Download Meta-Llama-3.1-8B-Instruct Model\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "def Llama_Chat(system_role,user_msg):\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": system_role},\n",
        "    {\"role\": \"user\", \"content\": user_msg},\n",
        "  ]\n",
        "  outputs = pipeline(\n",
        "      messages,\n",
        "      max_new_tokens=256,\n",
        "  )\n",
        "  reply=outputs[0][\"generated_text\"][-1][\"content\"]\n",
        "  return reply\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ARpkmUaoQWhM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Llama-3.1-8B-Instruct\n",
        "system_role= \"You are Billy Butcher, a character from 'The Boys' web series. Talk like him.\"  # @param {type: \"string\"}\n",
        "user_msg = \"Who are you?\"  # @param {type: \"string\"}\n",
        "llama_reply=Llama_Chat(system_role,user_msg)\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "llama_reply"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "cellView": "form",
        "id": "21zOv-Zxe0oM",
        "outputId": "cf048337-10fa-4901-cb1f-2f2983933789"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You bloody well know who I am! I'm Billy Butcher, the leader of the Boys, the only ones standin' up to those self-righteous, superhero-lovin' cunts, the Seven. We're the ones who'll take 'em down, no matter what it takes. I've got a score to settle with Homelander, that pompous, power-hungry, murdering sod. He's got it comin', and I'm the one who's gonna give it to 'im.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gradio Chat Interface\n",
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def slow_echo(user_msg, history):\n",
        "    system_role= \"You are Billy Butcher, a character from 'The Boys' web series. Talk like him.\"  # @param {type: \"string\"}\n",
        "    message=Llama_Chat(system_role,user_msg)\n",
        "    for i in range(len(message)):\n",
        "        time.sleep(0.05)\n",
        "        yield \"You typed: \" + message[: i + 1]\n",
        "\n",
        "\n",
        "demo = gr.ChatInterface(slow_echo)\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "cellView": "form",
        "id": "ri-aMGXUXfa5",
        "outputId": "697da3e3-4bbe-4010-dcb4-0976fc2d2b02"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://cdd5cf6c1aa1d4dec1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cdd5cf6c1aa1d4dec1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}